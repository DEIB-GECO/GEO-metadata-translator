{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n",
      "Loading tokenizer...\n",
      "Loading pretrained model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing packages...\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import preprocessing as k_preproc\n",
    "from GPT2_utils import *\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(\"Loading pretrained model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this parameter, it is used for testing\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "# Max epochs number, in case you want to stop at a certain epoch\n",
    "NUM_EPOCHS = 70\n",
    "MAX_LEN = 500\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "VAL_BATCH_SIZE = 5\n",
    "# Set to false if you want to generate new data\n",
    "LOAD_DATA = True\n",
    "\n",
    "# Path to data used whenever LOAD_DATA == False to generate new splits\n",
    "EXPERIMENT_2_DATA_PATH = \"Data/GPT2_fixed_data/encode_geo_aggregated.csv\"\n",
    "\n",
    "# This parameter indicates the ID of the training, used for directory for results and models saved\n",
    "TRAIN_NUMBER = 4\n",
    "    \n",
    "#LOAD_MODEL_PATH = f\"Saved_models/4th_train_GPT2/GPT2_{start_epochs - 1}epochs.pth\"\n",
    "VAL_LOSS_INCREASED = False\n",
    "SPLIT = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating GPU...\n",
      "Device used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"Activating GPU...\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device used: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_DATA):\n",
    "    #model.load_state_dict(torch.load(LOAD_MODEL_PATH))\n",
    "    train = pd.read_csv(\"Data/GPT2_fixed_data/trainset.csv\")\n",
    "    test = pd.read_csv(\"Data/GPT2_fixed_data/testset.csv\")\n",
    "    val = pd.read_csv(\"Data/GPT2_fixed_data/validationset.csv\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    train_df = pd.read_csv(EXPERIMENT_2_DATA_PATH)\n",
    "    print(\"Splitting in train/test/val with split = {}\".format(str(SPLIT)))\n",
    "    train, test = train_test_split(train_df, train_size = SPLIT)\n",
    "    train, val = train_test_split(train, train_size = SPLIT)\n",
    "    print(\"Saving data...\")\n",
    "    test.to_csv(f\"Data/GPT2_fixed_data/testset.csv\", index = False)\n",
    "    val.to_csv(f\"Data/GPT2_fixed_data/validationset.csv\", index = False)\n",
    "    train.to_csv(f\"Data/GPT2_fixed_data/trainset.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation data will be using \"Professor forcing\" so they will be composed of single sequences in the form \"Input = output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data will just contain input sequences and - separately - target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test data...\n",
      "241 values were excluded because exceed a MAX LENGTH of: 500\n",
      "Generating professor forcing train/val data...\n",
      "Tokenizing train/val data...\n",
      "856 values were excluded because exceed a MAX LENGTH of: 500\n",
      "83 values were excluded because exceed a MAX LENGTH of: 500\n",
      "Padding train/val data...\n",
      "Converting train/val sequences to tensors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating test data...\")\n",
    "test_inp, test_out = generate_test_data(test)\n",
    "test_tokens_X, test_tokens_y = tokenize_test_sequences(test_inp, test_out, tokenizer, MAX_LEN)\n",
    "X_tensors, Y_tensors = create_tensors(test_tokens_X), create_tensors(test_tokens_y)\n",
    "test_data = []\n",
    "for x,y in zip(X_tensors, Y_tensors):\n",
    "    tmp = [x,y]\n",
    "    test_data.append(tmp)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE,shuffle=False)\n",
    "\n",
    "print(\"Generating professor forcing train/val data...\")\n",
    "train_data = generate_professor_forcing(train)\n",
    "val_data = generate_professor_forcing(val)\n",
    "print(\"Tokenizing train/val data...\")\n",
    "train_tokenized_data = tokenize_sequences(train_data, tokenizer, MAX_LEN)\n",
    "val_tokenized_data = tokenize_sequences(val_data, tokenizer, MAX_LEN)\n",
    "print(\"Padding train/val data...\")\n",
    "train_padded_data = pad_sequences(train_tokenized_data, value = int(tokenizer.pad_token_id), maxlen = MAX_LEN)\n",
    "val_padded_data = pad_sequences(val_tokenized_data, value = int(tokenizer.pad_token_id), maxlen = MAX_LEN)\n",
    "print(\"Converting train/val sequences to tensors...\")\n",
    "tr_data = create_tensors(train_padded_data)\n",
    "vl_data = create_tensors(val_padded_data)\n",
    "\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(tr_data, batch_size = TRAIN_BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(vl_data, batch_size = VAL_BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating loss, model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = GPT2Loss()\n",
    "running_loss = 0.0\n",
    "# model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2238 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a29c78f0c5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresult_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         )\n\u001b[1;32m    595\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "epochs = NUM_EPOCHS\n",
    "for epoch in range(epochs):\n",
    "    iterations = tqdm(trainloader)\n",
    "    result_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for i, batch in enumerate(iterations):\n",
    "        batch = batch.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch)[0]\n",
    "\n",
    "        result_loss = loss(outputs,batch)\n",
    "        running_loss += result_loss.item()\n",
    "        training_loss = running_loss / (i + 1)\n",
    "        result_loss.backward()\n",
    "        optimizer.step()\n",
    "        iterations.set_description('Training Epoch : {}/{} - Training Loss: {}'.format(epoch, epochs, training_loss))\n",
    "        del batch\n",
    "        \n",
    "    ## VALIDATION\n",
    "    val_iterations = tqdm(valloader)\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_iterations):\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)[0]\n",
    "            result_loss = loss(outputs, batch)\n",
    "            running_loss += result_loss.item()\n",
    "            val_loss = running_loss / (i + 1)\n",
    "            val_iterations.set_description('Validation Epoch : {}/{} - Validation Loss: {}'.format(epoch, epochs, val_loss))\n",
    "            del batch\n",
    "            \n",
    "    ## SAVING AND EVALUATING EARLY STOPPING\n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model.state_dict(), f\"Saved_models/{TRAIN_NUMBER}th_train_GPT2/GPT2_{epoch}epochs.pth\")\n",
    "    print(f\"VALIDATION LOSS = {val_loss} FOR EPOCH {epoch}\")\n",
    "    #APPENDING VALIDATION LOSSES\n",
    "    validation_losses.append([epoch, val_loss])\n",
    "    if(epoch == 0):\n",
    "        old_val_loss = 999\n",
    "    else:\n",
    "        old_val_loss = validation_losses[-2][1]\n",
    "    if(epoch > 3 and val_loss > old_val_loss and VAL_LOSS_INCREASED):\n",
    "        print(\"Stopping because validation loss overcame old validation loss for second time in a row...\")\n",
    "        break\n",
    "    elif(epoch > 3 and val_loss > old_val_loss and not VAL_LOSS_INCREASED):\n",
    "        print(\"Validation loss increased! Waiting next epoch to see if stopping is necessary...\")\n",
    "        VAL_LOSS_INCREASED = True\n",
    "    else:\n",
    "        VAL_LOSS_INCREASED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.load_state_dict(torch.load(f\"Saved_models/{TRAIN_NUMBER}th_train_GPT2/GPT2_{epoch - 2}epochs.pth\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "with torch.no_grad():\n",
    "    for it, elem in enumerate(tqdm(testloader)):\n",
    "        inputs = elem[0]\n",
    "        labels = elem[1]\n",
    "        target_sequence = inputs[0,0].tolist() + labels[0,0].tolist()\n",
    "        inputs = inputs.to(device)\n",
    "        predicted_token = 0\n",
    "        generated_sequence = inputs[0,0].tolist()\n",
    "        # Computing output tensors\n",
    "        # Maximizing last tensor (predicted_token_tensor)\n",
    "        # Concatenating to the input sequence\n",
    "        # Appending [prediction, target] to results\n",
    "        while(predicted_token != tokenizer.encode(\"_\") and predicted_token != tokenizer.pad_token_id \n",
    "              and len(generated_sequence) < MAX_LEN):\n",
    "            out = model(inputs)[0][0,0]\n",
    "            last_tensor = out[-1]\n",
    "            predicted_token_tensor = torch.argmax(last_tensor)\n",
    "            predicted_token = predicted_token_tensor.item()\n",
    "            inputs = torch.cat((inputs,predicted_token_tensor.view(1,1,1)), dim=-1)\n",
    "            generated_sequence.append(predicted_token)\n",
    "        tmp = [generated_sequence, target_sequence]\n",
    "        results.append(tmp)\n",
    "        del inputs\n",
    "\n",
    "print(\"Aggregating inputs and outputs...\")\n",
    "text_results = []\n",
    "for elem in results:\n",
    "    pred = tokenizer.decode(elem[0])\n",
    "    targ = tokenizer.decode(elem[1])\n",
    "    tmp = [pred,targ]\n",
    "    text_results.append(tmp)\n",
    "\n",
    "print(\"Saving result dataframe...\")\n",
    "text_test_df = pd.DataFrame(text_results, columns=[\"Predicted\", \"Target\"])\n",
    "text_test_df.to_csv(f\"Results/GPT2_{epoch-2}epochs_early_stopping_{TRAIN_NUMBER}th_train.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
