{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing packages...\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import preprocessing as k_preproc\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading pretrained model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(\"Loading pretrained model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "USE_GPU = True\n",
    "\n",
    "TESTING_DATA = \"Data/manually_labelled_gsm_data.csv\"\n",
    "TEST_BATCH_SIZE = 1\n",
    "EPOCH_OF_MODEL_TO_LOAD = 26\n",
    "\n",
    "\n",
    "\n",
    "PATH_TO_MODEL = f\"../Experiment_2/Saved_models/1st_train_GPT2/GPT2_{EPOCH_OF_MODEL_TO_LOAD - 1}epochs.pth\"\n",
    "MAX_LEN = 500\n",
    "MAX_INPUT_LEN = 450\n",
    "\n",
    "test = pd.read_csv(TESTING_DATA)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(dataframe):\n",
    "    inputs = dataframe['Input'].values\n",
    "    targets = dataframe['GSM'].values\n",
    "    corrected_labels = targets\n",
    "    corrected_inputs = inputs\n",
    "    return corrected_inputs, corrected_labels\n",
    "\n",
    "def tokenize_test_sequences(x,y):\n",
    "\ttokenized_x = []\n",
    "\ttokenized_y = []\n",
    "\tfor _x,_y in zip(x,y):\n",
    "\t\tif(type(_x) == str):\n",
    "\t\t\tencoded_x = tokenizer.encode(_x)\n",
    "\t\t\tencoded_y = tokenizer.encode(\"LALA\")\n",
    "\t\t\tdim_x = len(encoded_x)\n",
    "\t\t\tif(dim_x <= MAX_LEN):\n",
    "\t\t\t\ttokenized_x.append(encoded_x)\n",
    "\t\t\t\ttokenized_y.append(encoded_y)\n",
    "\texcluded = len(x) - len(tokenized_x)\n",
    "\tprint(\"{} values were excluded because exceed a MAX LENGTH of: {}\".format(excluded,MAX_LEN))\n",
    "\treturn tokenized_x, tokenized_y\n",
    "\n",
    "def create_tensors(data):\n",
    "    tensor_list = []\n",
    "    max_len_tr = 0\n",
    "    for elem in data:\n",
    "        ids = torch.tensor(elem).unsqueeze(0)\n",
    "        tensor_list.append(ids)\n",
    "    return tensor_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for it, elem in enumerate(tqdm(testloader)):\n",
    "\n",
    "        inputs = elem\n",
    "        target_sequence = inputs[0,0].tolist()\n",
    "        inputs = inputs.to(device)\n",
    "        predicted_token = 0\n",
    "        generated_sequence = inputs[0,0].tolist()\n",
    "        # Computing output tensors\n",
    "        # Maximizing last tensor (predicted_token_tensor)\n",
    "        # Concatenating to the input sequence\n",
    "        # Appending [prediction, target] to results\n",
    "        while(predicted_token != tokenizer.encode(\"_\") and predicted_token != tokenizer.pad_token_id \n",
    "              and len(generated_sequence) < MAX_LEN):\n",
    "            out = model(inputs)[0][0,0]\n",
    "            last_tensor = out[-1]\n",
    "            predicted_token_tensor = torch.argmax(last_tensor)\n",
    "            predicted_token = predicted_token_tensor.item()\n",
    "            inputs = torch.cat((inputs,predicted_token_tensor.view(1,1,1)), dim=-1)\n",
    "            generated_sequence.append(predicted_token)\n",
    "        results.append(generated_sequence)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating text results...\")\n",
    "text_results = []\n",
    "for i in range(len(results)):\n",
    "    elem = results[i]\n",
    "    pred = tokenizer.decode(elem)\n",
    "    text_results.append([gsms[i],pred])\n",
    "\n",
    "print(\"Saving dataframes...\")\n",
    "result_frame = pd.DataFrame(text_results, columns = ['GSM','Prediction'])\n",
    "result_frame.to_csv(f\"Results/manually_labelled_gsm_results_{EPOCH_OF_MODEL_TO_LOAD}_epochs.csv\")\n",
    "# test['Output'] = text_results\n",
    "# test.to_csv(\"Results/GPT2/unlabelled_results_16-24epochs_onencode_and_geo_with_gsm.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
